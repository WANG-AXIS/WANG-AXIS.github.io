---
title: "WANG-AXIS Lab - Projects"
layout: textlay
excerpt: "WANG-AXIS Lab -- Projects"
sitemap: false
permalink: /projects/
---


<!-- This content will not appear in the rendered Markdown 

# Projects

## Artificial Intelligence / Machine Learning for Tomographic Imaging
*	**2016 [First perspective](https://ieeexplore.ieee.org/document/7733110) on machine learning / deep learning for tomographic imaging**, as a roadmap for the new area of “deep reconstruction” / “Deep imaging” and a basis for the 1st special journal issue on this theme (IEEE Trans. Medical Imaging (TMI)), recognized as 2018 [IEEE Access Featured Article](https://ieeeaccess.ieee.org/featured-articles/deepimaging/)

*	**2017 Top 10 most downloaded articles of Med. Phys.** (Wang G, Kalra M, Orton C: [Machine learning will transform radiology significantly within the next 5 years](https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.12204), Med. Phys. 44:2041-2044; by the end of 2017, it received 1727 downloads)

*	**2018 IEEE TMI featured special issue**: Wang G, Ye JC, Mueller K, Fessler JA: Image Reconstruction Is a New Frontier of Machine Learning — Editorial for the Special Issue “[Machine Learning for Image Reconstruction](https://ieeexplore.ieee.org/document/8359079)”. IEEE TMI, June, 2018

*	**2018 Invited NIH Talks**: “[Large, Public, Multimodal Image Datasets” and “Tomographic Reconstruction with Machine Learning](https://www.nibib.nih.gov/newsevents/meetings-events/artificial-intelligence-medical-imaging-workshop)”, Artificial Intelligence in Medical Imaging Workshop, NIH, Bethesda, MD, Aug. 23, 2018 

*	**2019 Roadmap article in Radiology**: “[A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop](https://pubs.rsna.org/doi/10.1148/radiol.2019190613)” by Langlotz CP et al. 

*  **A series of journal papers** on deep neural networks for low-dose, sparse-data, superresolution, and other CT topics, as well as for MRI and other imaging modalities

*	**Strategic partnerships funded by GE and other companies** in the machine learning based imaging areas since 2018; and **AI PhD Mentoring Grant funded by IBM** since 2019

*	**NIH R01 on deep radiomics with Memorial Sloan Kettering, NIH AIP/R01 on deep hybrid imaging with MARS Inc.** (rated Top 1%), funded in 2019, and **NIH R01 on deep cardiac CT with GE and Cornell**, to be funded

*	**Editorial Board Member for the IOP Journal “[Machine Learning: Science step-and-shootTechnology](https://iopscience.iop.org/journal/2632-2153)”**, 2019 

*	**Advisory Board Member of the Cell Journal “[Patterns](https://www.cell.com/patterns/home)”**, which is dedicated to data science and machine learning, 2020 

*	**2019 *Nature Machine Intelligence* Article**: “[Competitive performance of a modularized deep
neural network compared to commercial algorithms for low-dose CT image reconstruction](https://www.nature.com/articles/s42256-019-0057-9)” by Shan HM, Padole A, Homayounieh F, Kruger U, Khera RD, Nitiwarangkul C, Kalra MK, Wang G 

*	**2019 Graduate Course at RPI on Medicine Imaging in the Deep Learning Framework**,
which is the first of its kind in the world (https://ima.umn.edu/2019-2020/SW10.14-
18.19/28288)

*	**Wang G, et al., Textbook “[Machine Learning for Tomographic Imaging](https://iopscience.iop.org/book/978-0-7503-2216-4)” (410 pages), IOP, 2019** (The first and only book in this area, announced in “[Physics World](https://physicsworld.com/a/a-machine-learning-revolution/)”), published in 2019, [Link](https://physicsworld.com/a/machine-learning-for-tomographic-imaging)
-->

<br/>
<br/>
<br/>

# Projects in Progress


## Photon-counting CT and Optical Molecular Tomography

The goal is to develop a preclinical x-ray and optical prototype for High-dimensional Optical Tomography (HOT) Guided-by Energy-resolved Micro-CT (GEM), visualize breast tumor heterogeneity, HER2 expression and dimerization, and therapeutic response in mice.

<p align = "left">
<img src = "https://raw.githubusercontent.com/WANG-AXIS/wang-axis.github.io/master/images/projects/photon_counting.png" width="600">
</p>
<p align = "left">
</p>

<!--Photon-counting X-ray and Optical Tomography for Preclinical Cancer Research ABSTRACT Preclinical imaging is a critical tool in cancer research. Since cancer exhibits very complex spatiotemporal features, there is a strong need for the development of novel imaging technologies to characterize cancerous tissues and their microenvironments. For this purpose, multimodal imaging has the best potential to provide anatomical, functional and molecular information concurrently in live and intact animals. Of our primary interest, human epidermal growth factor receptor 2 (HER2) expression has prognostic and predictive values in breast cancer. Currently, therapeutic monoclonal anti-HER2 antibodies that inhibit receptor dimerization are FDA- approved. However, an increasingly more complex view of the role of HER2 in breast cancer has emerged from genome sequencing that highlights the importance of inter- and intra-tumor heterogeneity in therapy resistance. Thus, there is a clear need for a non-invasive preclinical imaging modality that is capable of monitoring the interplay between HER2 receptor expression level, targeted drug delivery, and tumor response. The overall goal of this project is to develop a hybrid x-ray and optical prototype for High-dimensional Optical Tomography (HOT) Guided-by Energy-resolved Micro-CT (GEM), visualize and quantitate breast tumor heterogeneity, HER2 expression and dimerization, and therapeutic response in preclinical models. On the x- ray side, photon-counting micro-CT records individual x-ray photons and their energy levels, and enables chemically-specific material decomposition. As a result, a mouse anatomy can be represented in terms of water, lipid, bone, Calcium, Iodine, and Gadolinium. On the optical side, optical molecular tomography maps the distribution of functional biomarkers and molecular probes. Of great importance to targeted therapy, with in vivo Macroscopy Fluorescence Lifetime Förster Resonance Energy Transfer (MFLI-FRET) imaging, our recent results demonstrate that quantitative MFLI-FRET signals correlate strongly with intracellular drug delivery at the pathological site as validated via ex vivo immunohistochemistry analysis. Synergistically, basis materials resolved with photon-counting micro-CT can be related to unique optical properties, and used to correct a heterogeneous optical background for quantitative optical molecular tomography. Furthermore, contrast- enhanced micro-CT can identify regions of interest to regularize optical molecular tomography. The specific aims are to (1) prototype a hybrid HOTGEM system for comprehensive and synergistic x-ray and optical imaging, (2) develop joint methods for image reconstruction from datasets in multi-contrasts collected with the HOTGEM system, and (3) characterize breast cancer in xenograft systems with varying levels of HER2 and HER2-activating mutations using the HOTGEM system. Upon completion, the proposed HOTGEM system will have been validated to offer 50µm x-ray resolution for material decomposition and 100µm optical resolution for target localization in co-registration within 30 minutes for each hybrid in vivo scan, demonstrated to be a breakthrough for tomographic HER2 imaging, and ready for technology transfer and commercial translation.-->

* Grant: NIH/NCI R01CA237267 
* Multi-PIs: Ge Wang\*, Xavier Intes, Barroso, Margarida (Albany Medical Center)
* Project schedule: 09/01/2019 – 08/31/2024


## Constrained Disentanglement for CT Metal Artifact Reduction

The goal is to develop deep learning algorithms for CT metal artifact reduction in the context of proton therapy.


<p align = "left">
<img src = "https://raw.githubusercontent.com/WANG-AXIS/wang-axis.github.io/master/images/projects/constrained_disentanglement.png" width="600">
</p>
<p align = "left">
</p>

<!--The World Health Organization reported that cancer is the second leading cause of death globally and is re- sponsible for 9.6 million deaths in 2018. Approximately 50% of all cancer patients receive radiation therapy (RT). Many of them have metal implants, which induce image artifacts in the treatment planning CT images and compromise or preclude treatment in an estimated 15% of all radiation therapy patients. Despite extensive CT metal artifact reduction (MAR) research it remains one of the long-standing challenges in the CT field, without a clinically satisfactory solution. The overall goal of this project is to develop cutting-edge deep learning imaging methods and software solutions for commercial CT scanners to eliminate CT metal artifacts in general and improve RT in particular. We propose a three-pronged approach to systematically tackle this challenge in three specific aims: (1) adversarial learning techniques for estimation of sinogram missing data and metal traces; (2) constrained disentanglement (CODE) networks to remove CT image artifacts during image reconstruction, through post-processing, and in both data and image domains; and (3) systematic evaluation of our proposed CT MAR techniques and clinical translation into robust RT planning methods to maximize the RT treatment planning accuracy and thus improve patient outcomes. Our synergistic track records in CT MAR research, especially with deep imaging methods over the past three years, promises an unprecedented opportunity for a brand-new solution to CT MAR. For the first time we will integrate contemporary AI innovations in data preprocessing, image reconstruction, post-processing, observer studies and treatment planning synergistically in a unified data-driven framework, positioning this project uniquely to eliminate metal artifacts and their complications in radiation therapy. This project will be pursued through the long-term academic-industrial partnership among Dr. Ge Wang at Ren- sselaer Polytechnic Institute (RPI), Dr. Bruno De Man at GE Research Center (GRC), and Dr. Harald Paganetti at Massachusetts General Hospital (MGH). While our teams will collaborate closely through the whole project, GRC has a history of CT research and translation, including direct raw data processing, and will focus on Aim 1. RPI is a pioneering group in tomographic reconstruction, especially deep-learning-based CT imaging, and will lead Aim 2. The MGH team is at the forefront of radiation therapy research and will be responsible for Aim 3. Upon completion of this project, we will have redefined the state of the art of CT MAR, largely eliminating CT metal artifacts and substantially improving radiation therapy planning and delivery accuracy. With the above-proposed networks for CT MAR, metal artifacts will have been basically eliminated, targeting residual errors <10 HU for photon and proton therapy planning, with the goal of reducing the clinical diametric error to ±3% and the proton range error due to metal artifacts to <2mm. Since our approach is software-based and open-source, the path for technology transfer and clinical translation is clearly defined, as well tested before.-->

* Grant: NIH/NIBIB R01EB031102
* Multi-PIs: Ge Wang\*, Bruno De Man (GE), Harald Paganetti (Harvard)
* Project schedule: 09/01/2021 – 08/31/2024



## Adversarially Based Virtual CT Workflow for Evaluation of AI Imaging

The goal is to develop an FDA workflow evaluating, approving, and monitoring AI imaging software. This project would be a key element in the future healthcare metaverse.


<p align = "left">
<img src = "https://raw.githubusercontent.com/WANG-AXIS/wang-axis.github.io/master/images/projects/adversarially_based_ct.png" width="600">
</p>
<p align = "left">
</p>

<!--Adversarially Based Virtual CT Workflow for Evaluation of AI in Medical Imaging ABSTRACT Over the past several years, artificial intelligence (AI) and machine learning (ML), especially deep learning (DL), has been the most prominent direction of tomographic research, commercial development, clinical translation, and FDA evaluation. Recently, it has become widely recognized that deep neural networks often have generalizability issues and are vulnerable to adversarial attacks, deliberate or unintentional. This critical challenge must be addressed to optimize the performance of deep neural networks in medical applications. In January this year, FDA published an action plan for furthering the oversight for AI/DL-based software as medical devices (SaMDs). One major action underlined in the plan is “regulatory science methods related to algorithm bias and robustness”. The significance of ensuring the safety and effectiveness of AI/DL-based SaMDs cannot be overestimated since AI is expected to play a critical role in the future of medicine. In this context, the overall goal of this academic-FDA partnership R01 project is to generate diverse training and challenging testing datasets of low-dose CT (LDCT) scans, prototype a virtual CT workflow, and establish an evaluation methodology for AI-based imaging products to support FDA marketing authorization. The technical innovation lies in cutting-edge DL methods empowered by (a) adversarial learning to generate anatomically and pathologically representative features in the human chest; (b) adversarial attacking to probe the virtual CT workflow in individual steps and its entirety; and (c) systematic evaluation methods to better characterize and predict the clinical performance of AI-based imaging products. In contrast to other CT simulation pipelines, our Adversarially Based CT (ABC) platform relies on adversarial learning to ensure diversity and realism of the simulated data and images and improve the generalizability of deep networks, and utilizes adversarial samples to probe the ABC workflow to address the robustness of deep networks. The overarching hypothesis is that adversarial learning and attacking methods are powerful to deliver high- quality datasets for AI-based imaging research and performance evaluation. The specific aims are: (1) diverse patient modeling (SBU), (2) virtual CT scanning (UTSW), (3) deep CT imaging (RPI), (4) virtual workflow validation (FDA), and (5) ABC system dissemination (RPI-SBU-UTSW-FDA). In this project, generative adversarial learning will play an instrumental role in generating features of clinical semantics. Also, adversarial samples will be produced in both sinogram and image domains. In these complementary ways, AI-based imaging products can be efficiently evaluated for not only accuracy but also generalizability and robustness. Upon completion, our ABC workflow/platform will be made publicly available and readily extendable to other imaging modalities and other diseases. This ABC system will be shared through the FDA’s Catalog of Regulatory Science Tools, and uniquely well positioned to greatly facilitate the development, assessment and translation of emerging AI-based imaging products.-->


* Grant: NIH/NIBIB R01EB032716
* Multi-PIs: Ge Wang\*, Klaus Mueller (Stony Brook), Xun Jia (JHU), Rongping Zeng (FDA)
* Project schedule: 04/01/2022 – 03/31/2026


## Cardiac CT Deblooming 

The goal is to use deep learning to eliminate blooming artifacts in cardiac CT images without costly redesign of the CT hardware. This will be based on a dual-domain workflow from sinogram processing through image reconstruction to image analysis.


<p align = "left">
<img src = "https://raw.githubusercontent.com/WANG-AXIS/wang-axis.github.io/master/images/projects/cardiac_ct_deblooming.png" width="600">
</p>
<p align = "left">
</p>

<!--Coronary artery disease (CAD) is the most common type of heart disease, killing over 370,000 Americans annu- ally2. Cardiac CT is a safe, accurate, non-invasive method widely employed for diagnosis of CAD and planning therapeutic interventions. With the current CT technology, calcium blooming artifacts severely limit the accuracy of coronary stenosis assessment. Similarly, stent blooming artifacts lead to overestimation of in-stent restenosis. As a result, many coronary CT angiography (CCTA) scans are non-diagnostic and result in patients receiving costly and invasive coronary angiography (ICA) procedures. Based on extensive feasibility results, the goal of this project is to use deep learning innovations to fundamen- tally eliminate blooming artifacts without costly redesign of the CT hardware. A consortium between GE Re- search, Rensselaer Polytechnic Institute and Weill Cornell Medicine will develop dedicated imaging protocols and machine learning methods to avoid or minimize blooming artifacts and evaluate the clinical impact of the proposed solutions. In Aim 1, the CT scan protocol will be optimized and paired with deep learning reconstruc- tion and post-processing algorithms to generate high-resolution CT images and prevent blooming artifacts. In Aim 2, image-domain and raw-data-domain deep learning processing algorithms will be developed to correct for residual blooming. After successful demonstration of the proposed methods on phantom scans and emulated clinical datasets, in Aim 3 the proposed CT methods will be clinically demonstrated and optimized based on 100 patients with coronary artery disease, using intravascular ultrasound as the ground-truth reference. At the end of the project, we will have demonstrated and publicly disseminated a systematic methodology to essentially remove blooming artifacts in cardiac CT without a costly hardware upgrade. This will be another suc- cess of deep learning, enabling accurate coronary stenosis assessment and eliminating many unnecessary diag- nostic catheterizations.-->

* Grant: NIH/NIBIB R01HL151561
* Multi-PIs: Bruno De Man\* (GE), Ge Wang, James Min (Cleerly/Cornell)
* Project schedule: 04/01/2020 – 06/30/2024


## SPECT with a Compton Camera for Thyroid Cancer Imaging

This goal is to design a high-efficiency and high-quality Compton camera based tomographic imaging system.

<!--SPECT with a Compton Camera for Thyroid Cancer Imaging ABSTRACT The thyroid gland is butterfly-shaped in the lower front of the neck, and secretes hormones for normal biological functions. The incidence of thyroid nodules increases with age, involving more than half of the population. Thyroid cancer is the most common type of endocrine-related cancer and the most common cancer in young women, with over 50K new cases per year in the United States. To detect and treat thyroid cancer, it is desired to characterize the nodule accurately. Currently, single photon emission computed tomography (SPECT) and computed tomography (CT) are used with radioiodine scintigraphy to evaluate patients with thyroid cancer. The gamma camera for SPECT contains a mechanical collimator that greatly compromises dose efficiency and limits diagnostic sensitivity. Fortunately, the Compton camera is emerging as an ideal approach for mapping the distribution of radiopharmaceuticals inside the thyroid. It is because the Compton camera requires no mechanical collimation and in principle rejects no gamma ray photon. Hence, radiation dose will be reduced by orders of magnitude in screening and follow-up scans of patients. In this R21 project, we will design a high-efficiency and high-quality tomographic imaging system with a Compton camera dedicated to thyroid cancer imaging, and develop an associated software package for Compton scattering based SPECT imaging. The major innovation lies in the deep learning empowered image reconstruction and the Timepix3-based Compton camera for thyroid cancer imaging. The proposed techniques help reduce radiation dose dramatically, improve the imaging speed, and enhance image quality and diagnostic performance, having a great potential for clinical translation. The three specific aims are defined as follows: (1) a Monte Carlo simulator will be developed for gamma ray Compton data synthesis; (b) deep reconstruction algorithms will be developed for Compton camera based SPECT, and (c) a SPECT system will be designed in numerical simulation and phantom experiments for ultra-low-dose thyroid imaging. Upon the completion of this project, the simulation and reconstruction software tools should have been developed for tomographic imaging of the radiotracer distribution in the human thyroid, and a point of care (POC) SPECT system will have been designed with the Compton camera and experimentally verified for a superior diagnostic performance at an ultra-low dose. The synergy among the deep learning techniques and the cutting-edge Timepix3 camera will have been demonstrated for a follow-up R01 proposal.-->

* Grant: NIH/NCI R21CA264772
* MPI: Wang
* Project schedule: 05/01/2021 – 04/30/2023



## Bio-tissue Oxygenation Nanophosphor Enabled Sensing (BONES)

The goal is to develop radiomics/rawdiomics for personalized treatment of colorectal liver metastases.

* Grant: NIH/NIGMS R42GM142394
* Sub-PI: Wang
* Project schedule: 06/01/2021 – 11/30/2023



## Focused X-ray Luminescence Tomography

The goal is to develop the first-of-its-kind focused x-ray luminescence tomography (FXLT) scanner to “slice” deep cancers/tissues sensitively and longitudinally at 150μm resolution at a radiation dose comparable to that of a regular micro-CT scan.

* Grant: NIH/NIBIB R01EB026646 
* Sub-PI: Wang
* Project schedule: 07/01/2018 – 03/31/2022



## Radiomic Markers of Response and Recurrence for Cancer Patients

The goal is to develop and validate robust imaging features by standardizing image acquisition, improve automated tools for clinical trial use, and validate the predictive power of imaging features with external data.

* Grant: NIH/NCI R01CA233888
* Sub-PI: Wang
* Project schedule: 03/01/2019 – 08/31/2024


## Deep Learning for Medical CT

The goal is to develop medical CT algorithms.

* Industrial grant: General Electric (USA)
* PI: Wang
* Project schedule: 01/01/2021– 12/31/2022


## Preclinical Micro-CT

The goal is to develop preclinical micro-CT algorithms.

* Industrial grant: FirstImaging (Shanghai, China)
* PI: Wang
* Project schedule: 01/01/2021– 12/31/2021


<br/>
<br/>
<br/>



# Projects in Preparation


## Temporal Bone CT With Interior, Deep and Robotic Imaging

The goal is to develop a robotic clinical micro-CT system with cutting-edge x-ray source and photon-counting detector technologies for temporal bone imaging.

* Grant from NIH/NIBIB
* Contact PI: Wang
* Project Schedule: 04/01/2023-03/31/2027


## Auto-driving Vehicle for Affordable Tomo-Analytic Robots (AVATAR)

<!--Given unprecedented progresses in the engineering field over the past decade or so, the development of AVATAR is timely to integrate cutting-edge machine learning, auto-driving, medical imaging, robot, computer vision, virtual/mixed reality, high-performance computing, and internet technologies, and change the landscape of the imaging world. This is particularly helpful for cancer screening, diagnosis, and follow-up in underdeveloped countries. We are open to collaborate with those who are interested to address the healthcare needs in low-middle income countries (LMIC).-->

For more details, please click the following links:

*	The [white paper on AVATAR](http://biotech.rpi.edu/sites/default/files/AVATAR-RPI.pdf) used in an RPI's internal session for brainstorming about potential engineering research centers (ERC);
*	The [bilingual blog on AVATAR](https://mp.weixin.qq.com/s/dsZnqY-CeDYn1uBrZAFIXw) in Chinese (upper part) and English (lower part) respectively;
*	Our relevant work on [how to create a low-cost CT scanner](http://live.iop-pp01.agh.sleek.net/2015/01/28/how-to-create-a-low-cost-ct-scanner/);
*	Our relevant work on [how to create a cost-effective yet flexible hybrid x-ray and MR imager (MRX)](https://ieeexplore.ieee.org/document/7779075/);
*	Those who are interested, please contact [Dr. Ge Wang](http://biotech.rpi.edu/centers/bic/people/faculty/ge-wang) for further discussion.


<br/>
<br/>
<br/>


<!-- This content will not appear in the rendered Markdown 

# Projects (Completed)

## Cone-beam Spiral CT 

*	CT plays a central role in imaging, mainly achieved with spiral/helical cone-beam/multi-slice scanning. Ge Wang pioneered the first spiral cone-beam CT algorithms in 1991, has systematically working  on this topic with major and lasting impacts, and been recognized as Fellow of National Academy of Inventors. There are > 100M medical scans yearly, with a majority in the [spiral/helical cone-beam/multi-slice scanning mode](http://www.rdmag.com/award-winners/2014/08/flattening-yields-faster-ct) . Also, there are over 500-million [spiral cone-beam/multi-slice airport CT scans](http://www.dhs.gov/blog/2014/01/24/tsa-blog-year-review-2013). [Spiral cone-beam CT methods](http://iopscience.iop.org/0031-9155/52/6/R01) have been extensively studied since 1991 (~2-million Google hits under alternative phrases) to solve this "long object problem" (longitudinal data truncation). 


## Fan-beam Spiral CT (Simpler/special case of cone-beam spiral CT)
*	A series of publications (1994-1997) establishing the superiority of spiral/helical fan-beam CT over the step-and-shoot scan mode that had been popular since the advent of CT. Our work was remarked in an editorial as “the state of the art in spiral CT” (Kalender, Radiology 197:578-580, 1995), having played a significant role in accelerating the transition into the spiral CT era.



## Interior Tomography & Omni-Tomography
*	[Breakthrough papers on interior tomography](http://iopscience.iop.org/0031-9155/58/16/R161) to solve the long-standing “interior problem” (transverse data truncation) (2007-2013), with major practical benefits .
*	General interior tomography as a guiding principle that enables omni-tomography (2011) (“all-in-one”) to acquire different datasets (“all-at-once”), with simultaneous CT-MRI and simultaneous CT-SPECT as examples [[ref1](http://iopscience.iop.org/0031-9155/58/16/R161), [ref2](http://medicalphysicsweb.org/cws/article/opinion/51026), [ref3](http://spectrum.ieee.org/biomedical/imaging/path-found-to-a-combined-mri-and-ct-scanner)]


## Bioluminescence Tomography & Luminescence Tomography
*	[Initial papers](http://www.nature.com/nbt/journal/v23/n3/pdf/nbt1074.pdf) on bioluminescence tomography (BLT) (2004), featured in Nature Biotechnology .
*	X-ray micro-modulated luminescence tomography (XMLT) (2014) as a new approach for microscopic imaging via combination of x-ray focusing and x-ray-induced luminescence emission.


## Bibliometrics
*	[Axiomatic index (A-index)](http://www.ncbi.nlm.nih.gov/pubmed/23720314) for quantification of individual academic credits to improve h- and other indices. This methodology was featured in [PNAS](http://www.pnas.org/content/110/24/9615.extract), and used by [AMiner](http://arnetminer.org). Its application in assessing any NIH review bias was reported in [Nature](http://www.nature.com/news/seven-days-1-7-february-2013-1.12364) and [Science](http://news.sciencemag.org/scienceinsider/2013/02/new-analysis-challenges-study-su.html).

-->



